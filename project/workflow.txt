generate cluster
    - basic config file
    - starts cluster with provider (command: cms cluster --provider=xxx N)
    - generate hadoop & spark image (lightweight)
    - master node (unchanging)

manage workers (workflow-driven)
    - external, use cloud api to create machines and add to cluster
    - add required image to node (based on workflow)
    - run job on cluster with parameter (input/output, etc)

add job
    - add job to workflow in orchestrator

deploy job
    - deploy job to cluster


manage job
    - manage job status in orchestrator


collect output
    - manage all parameters passed to nodes from each job and be able to call the outputs written to those files
    